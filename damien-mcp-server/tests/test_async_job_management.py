"""
Comprehensive Test Suite for Async Job Management Infrastructure

This test suite validates the complete job lifecycle management for the
Damien Email Wrestler platform, building on the proven async infrastructure
that has successfully processed 3,500 emails in 7 minutes with 90% reliability.

Test Coverage:
- Job creation and initialization
- Progress tracking and status monitoring 
- Job cancellation and cleanup
- Error handling for invalid requests
- Job listing and active task management
- Resource management and memory cleanup
"""

import pytest
import asyncio
import time
from unittest.mock import Mock, AsyncMock, patch
from datetime import datetime

# Import the components under test
from app.services.async_processor import AsyncTaskProcessor, TaskStatus, AsyncTask
from app.tools.async_tools import (
    damien_ai_analyze_emails_async_handler,
    damien_job_get_status_handler,
    damien_job_get_result_handler,
    damien_job_cancel_handler,
    damien_job_list_handler
)


class TestAsyncTaskProcessor:
    """Test the core AsyncTaskProcessor functionality."""
    
    @pytest.fixture
    def processor(self):
        """Create a fresh AsyncTaskProcessor for each test."""
        return AsyncTaskProcessor()
    
    @pytest.fixture
    def simple_task_func(self):
        """Simple test function that returns a result."""
        async def task_func(params):
            await asyncio.sleep(0.1)  # Simulate processing
            return {"result": "success", "data": params.get("data", "test")}
        return task_func
    
    @pytest.fixture
    def slow_task_func(self):
        """Slower test function for cancellation testing."""
        async def task_func(params):
            await asyncio.sleep(2.0)  # Longer processing
            return {"result": "completed"}
        return task_func
    
    @pytest.fixture
    def failing_task_func(self):
        """Task function that raises an exception."""
        async def task_func(params):
            await asyncio.sleep(0.1)
            raise ValueError("Test error for validation")
        return task_func

    @pytest.mark.asyncio
    async def test_task_submission_and_completion(self, processor, simple_task_func):
        """Test successful task submission and completion."""
        # Submit task
        task_id = await processor.submit_task(
            name="Test Task",
            processor_func=simple_task_func,
            parameters={"data": "test_value"}
        )
        
        # Verify task was created
        assert task_id.startswith("task_")
        assert task_id in processor.active_tasks
        
        # Wait for completion
        await asyncio.sleep(0.2)
        
        # Verify task completed and moved to completed tasks
        assert task_id not in processor.active_tasks
        assert task_id in processor.completed_tasks
        
        # Check status
        status = processor.get_task_status(task_id)
        assert status is not None
        assert status["status"] == TaskStatus.COMPLETED.value
        assert status["progress"] == 100.0
        assert status["result"]["result"] == "success"
        assert status["result"]["data"] == "test_value"

    @pytest.mark.asyncio
    async def test_task_progress_tracking(self, processor, simple_task_func):
        """Test that task progress is tracked correctly."""
        task_id = await processor.submit_task(
            name="Progress Test",
            processor_func=simple_task_func,
            parameters={"data": "progress_test"}
        )
        
        # Check initial status (should be running quickly)
        await asyncio.sleep(0.05)
        status = processor.get_task_status(task_id)
        assert status["status"] in [TaskStatus.PENDING.value, TaskStatus.RUNNING.value]
        
        # Wait for completion and check final status
        await asyncio.sleep(0.2)
        status = processor.get_task_status(task_id)
        assert status["status"] == TaskStatus.COMPLETED.value
        assert status["start_time"] is not None
        assert status["end_time"] is not None

    @pytest.mark.asyncio
    async def test_task_cancellation(self, processor, slow_task_func):
        """Test task cancellation functionality."""
        # Submit a slow task
        task_id = await processor.submit_task(
            name="Cancellation Test",
            processor_func=slow_task_func,
            parameters={}
        )
        
        # Verify task is running
        await asyncio.sleep(0.1)
        assert task_id in processor.active_tasks
        
        # Cancel the task
        cancelled = processor.cancel_task(task_id)
        assert cancelled is True
        
        # Verify task was cancelled
        assert task_id not in processor.active_tasks
        assert task_id in processor.completed_tasks
        
        status = processor.get_task_status(task_id)
        assert status["status"] == TaskStatus.CANCELLED.value
        assert "cancelled by user" in status["message"]

    @pytest.mark.asyncio
    async def test_task_error_handling(self, processor, failing_task_func):
        """Test error handling for failing tasks."""
        task_id = await processor.submit_task(
            name="Error Test",
            processor_func=failing_task_func,
            parameters={}
        )
        
        # Wait for task to fail
        await asyncio.sleep(0.2)
        
        # Verify task failed properly
        assert task_id not in processor.active_tasks
        assert task_id in processor.completed_tasks
        
        status = processor.get_task_status(task_id)
        assert status["status"] == TaskStatus.FAILED.value
        assert "Test error for validation" in status["error"]
        assert "Task failed" in status["message"]

    def test_invalid_task_status_request(self, processor):
        """Test handling of invalid task ID requests."""
        status = processor.get_task_status("invalid_task_id")
        assert status is None

    def test_cancel_nonexistent_task(self, processor):
        """Test cancellation of non-existent task."""
        cancelled = processor.cancel_task("nonexistent_task")
        assert cancelled is False

    @pytest.mark.asyncio
    async def test_active_task_listing(self, processor, slow_task_func):
        """Test listing of active tasks."""
        # Submit multiple tasks
        task_ids = []
        for i in range(3):
            task_id = await processor.submit_task(
                name=f"List Test {i}",
                processor_func=slow_task_func,
                parameters={"index": i}
            )
            task_ids.append(task_id)
        
        # Check active tasks list
        await asyncio.sleep(0.1)
        active_tasks = processor.list_active_tasks()
        assert len(active_tasks) == 3
        
        # Verify all tasks are in the list
        active_task_ids = [task["task_id"] for task in active_tasks]
        for task_id in task_ids:
            assert task_id in active_task_ids
        
        # Cancel all tasks for cleanup
        for task_id in task_ids:
            processor.cancel_task(task_id)


class TestAsyncToolHandlers:
    """Test the async tool handlers that interface with MCP."""
    
    @pytest.fixture
    def mock_cli_bridge(self):
        """Mock CLI bridge for testing without actual email processing."""
        mock_bridge = AsyncMock()
        mock_bridge.ensure_initialized = AsyncMock()
        mock_bridge.fetch_emails = AsyncMock(return_value={
            "emails": [{"id": "1", "subject": "Test"}, {"id": "2", "subject": "Test 2"}],
            "total_fetched": 2
        })
        mock_bridge.analyze_email_patterns = AsyncMock(return_value={
            "patterns": [{"name": "Test Pattern", "count": 1}],
            "pattern_coverage_percentage": 85.0
        })
        mock_bridge.generate_business_insights = AsyncMock(return_value={
            "insights": ["Test insight"],
            "time_savings_hours": 5.0
        })
        return mock_bridge

    @pytest.mark.asyncio
    async def test_async_email_analysis_handler_success(self, mock_cli_bridge):
        """Test successful async email analysis handler."""
        with patch("app.tools.async_tools.async_processor") as mock_processor:
            mock_processor.submit_task = AsyncMock(return_value="test_job_123")
            
            params = {
                "days": 7,
                "target_count": 100,
                "min_confidence": 0.8,
                "query": "is:unread"
            }
            
            result = await damien_ai_analyze_emails_async_handler(params, {})
            
            assert result["success"] is True
            assert result["job_id"] == "test_job_123"
            assert result["status"] == "started"
            assert "Background analysis started for 100 emails" in result["message"]
            assert result["estimated_duration_minutes"] == 1  # 100 emails = 1 minute
            
            # Verify tracking instructions are provided
            assert "use_damien_job_get_status" in result["tracking"]
            assert "use_damien_job_get_result" in result["tracking"]

    @pytest.mark.asyncio
    async def test_job_status_handler_success(self):
        """Test successful job status retrieval."""
        with patch("app.tools.async_tools.async_processor") as mock_processor:
            mock_status = {
                "status": "running",
                "progress": 45.0,
                "message": "Processing emails...",
                "start_time": "2024-01-01T10:00:00",
                "end_time": None
            }
            mock_processor.get_task_status = Mock(return_value=mock_status)
            
            params = {"job_id": "test_job_123"}
            result = await damien_job_get_status_handler(params, {})
            
            assert result["success"] is True
            assert result["job_id"] == "test_job_123"
            assert result["status"] == "running"
            assert result["progress"]["percentage"] == 45.0
            assert result["progress"]["message"] == "Processing emails..."

    @pytest.mark.asyncio
    async def test_job_status_handler_job_not_found(self):
        """Test job status handler with non-existent job."""
        with patch("app.tools.async_tools.async_processor") as mock_processor:
            mock_processor.get_task_status = Mock(return_value=None)
            
            params = {"job_id": "nonexistent_job"}
            result = await damien_job_get_status_handler(params, {})
            
            assert result["success"] is False
            assert "Job nonexistent_job not found" in result["error"]

    @pytest.mark.asyncio
    async def test_job_status_handler_missing_job_id(self):
        """Test job status handler with missing job_id parameter."""
        params = {}
        result = await damien_job_get_status_handler(params, {})
        
        assert result["success"] is False
        assert "job_id parameter is required" in result["error"]

    @pytest.mark.asyncio
    async def test_job_result_handler_success(self):
        """Test successful job result retrieval."""
        with patch("app.tools.async_tools.async_processor") as mock_processor:
            mock_status = {
                "status": TaskStatus.COMPLETED.value,
                "result": {
                    "emails_analyzed": 150,
                    "patterns_detected": 3,
                    "insights": {"time_savings": "2 hours/week"}
                }
            }
            mock_processor.get_task_status = Mock(return_value=mock_status)
            
            params = {"job_id": "completed_job_123"}
            result = await damien_job_get_result_handler(params, {})
            
            assert result["success"] is True
            assert result["job_id"] == "completed_job_123"
            assert result["status"] == "completed"
            assert result["result"]["emails_analyzed"] == 150
            assert result["result"]["patterns_detected"] == 3

    @pytest.mark.asyncio
    async def test_job_result_handler_job_not_completed(self):
        """Test job result handler with job that's still running."""
        with patch("app.tools.async_tools.async_processor") as mock_processor:
            mock_status = {
                "status": TaskStatus.RUNNING.value
            }
            mock_processor.get_task_status = Mock(return_value=mock_status)
            
            params = {"job_id": "running_job_123"}
            result = await damien_job_get_result_handler(params, {})
            
            assert result["success"] is False
            assert "is not completed yet" in result["error"]
            assert "Current status: running" in result["error"]

    @pytest.mark.asyncio
    async def test_job_cancel_handler_success(self):
        """Test successful job cancellation."""
        with patch("app.tools.async_tools.async_processor") as mock_processor:
            mock_processor.cancel_task = Mock(return_value=True)
            
            params = {"job_id": "cancel_job_123"}
            result = await damien_job_cancel_handler(params, {})
            
            assert result["success"] is True
            assert result["job_id"] == "cancel_job_123"
            assert result["status"] == "cancelled"
            assert "has been cancelled" in result["message"]

    @pytest.mark.asyncio
    async def test_job_cancel_handler_job_not_found(self):
        """Test job cancellation with non-existent job."""
        with patch("app.tools.async_tools.async_processor") as mock_processor:
            mock_processor.cancel_task = Mock(return_value=False)
            
            params = {"job_id": "nonexistent_job"}
            result = await damien_job_cancel_handler(params, {})
            
            assert result["success"] is False
            assert "not found or already completed" in result["error"]

    @pytest.mark.asyncio
    async def test_job_list_handler_success(self):
        """Test successful job listing."""
        with patch("app.tools.async_tools.async_processor") as mock_processor:
            mock_active_tasks = [
                {"task_id": "job1", "name": "Analysis 1", "status": "running"},
                {"task_id": "job2", "name": "Analysis 2", "status": "pending"}
            ]
            mock_processor.list_active_tasks = Mock(return_value=mock_active_tasks)
            
            params = {}
            result = await damien_job_list_handler(params, {})
            
            assert result["success"] is True
            assert result["count"] == 2
            assert len(result["active_jobs"]) == 2
            assert result["active_jobs"][0]["task_id"] == "job1"
            assert result["active_jobs"][1]["task_id"] == "job2"

    @pytest.mark.asyncio
    async def test_error_handling_in_handlers(self):
        """Test error handling in async tool handlers."""
        # Test error in email analysis handler
        with patch("app.tools.async_tools.async_processor") as mock_processor:
            mock_processor.submit_task = AsyncMock(side_effect=Exception("Test error"))
            
            params = {"days": 7, "target_count": 100}
            result = await damien_ai_analyze_emails_async_handler(params, {})
            
            assert result["success"] is False
            assert "Failed to start async analysis" in result["error"]
            assert "Test error" in result["error"]


class TestJobLifecycleIntegration:
    """Integration tests for complete job lifecycle scenarios."""
    
    @pytest.mark.asyncio
    async def test_complete_job_lifecycle(self):
        """Test a complete job from creation to completion."""
        processor = AsyncTaskProcessor()
        
        # Define a test task that simulates email analysis
        async def mock_email_analysis(params):
            await asyncio.sleep(0.1)  # Simulate processing time
            return {
                "emails_analyzed": params.get("target_count", 100),
                "patterns_detected": 2,
                "processing_time": 0.1
            }
        
        # 1. Submit job
        task_id = await processor.submit_task(
            name="Integration Test Job",
            processor_func=mock_email_analysis,
            parameters={"target_count": 50}
        )
        
        # 2. Check initial status
        status = processor.get_task_status(task_id)
        assert status["status"] in [TaskStatus.PENDING.value, TaskStatus.RUNNING.value]
        
        # 3. Wait for completion
        await asyncio.sleep(0.2)
        
        # 4. Check completion status
        status = processor.get_task_status(task_id)
        assert status["status"] == TaskStatus.COMPLETED.value
        assert status["result"]["emails_analyzed"] == 50
        assert status["result"]["patterns_detected"] == 2
        
        # 5. Verify job moved from active to completed
        assert task_id not in processor.active_tasks
        assert task_id in processor.completed_tasks

    @pytest.mark.asyncio
    async def test_multiple_concurrent_jobs(self):
        """Test multiple jobs running concurrently."""
        processor = AsyncTaskProcessor()
        
        async def mock_analysis(params):
            job_id = params.get("job_id", "unknown")
            await asyncio.sleep(0.1)
            return {"job_id": job_id, "result": "completed"}
        
        # Submit 3 concurrent jobs
        task_ids = []
        for i in range(3):
            task_id = await processor.submit_task(
                name=f"Concurrent Job {i}",
                processor_func=mock_analysis,
                parameters={"job_id": i}
            )
            task_ids.append(task_id)
        
        # Verify all jobs are active
        await asyncio.sleep(0.05)
        active_tasks = processor.list_active_tasks()
        assert len(active_tasks) >= 3  # May complete quickly
        
        # Wait for all to complete
        await asyncio.sleep(0.2)
        
        # Verify all completed successfully
        for task_id in task_ids:
            status = processor.get_task_status(task_id)
            assert status["status"] == TaskStatus.COMPLETED.value
            assert status["result"]["result"] == "completed"

    @pytest.mark.asyncio
    async def test_memory_cleanup_with_many_tasks(self):
        """Test that completed tasks are cleaned up to prevent memory leaks."""
        processor = AsyncTaskProcessor()
        processor.max_completed_tasks = 5  # Set low limit for testing
        
        async def quick_task(params):
            return {"task": params.get("task_num")}
        
        # Submit many tasks to trigger cleanup
        task_ids = []
        for i in range(10):
            task_id = await processor.submit_task(
                name=f"Cleanup Test {i}",
                processor_func=quick_task,
                parameters={"task_num": i}
            )
            task_ids.append(task_id)
        
        # Wait for all to complete
        await asyncio.sleep(0.2)
        
        # Verify cleanup happened
        assert len(processor.completed_tasks) <= processor.max_completed_tasks
        assert len(processor.active_tasks) == 0


if __name__ == "__main__":
    # Run the tests
    pytest.main([__file__, "-v", "--tb=short"])
                f"{api_base_url}/tools/execute",
                headers=auth_headers,
                json=list_request,
                timeout=10.0
            )
            
            assert list_response.status_code == 200
            list_result = list_response.json()
            assert list_result.get("success") is True
            
            active_jobs = list_result.get("active_jobs", [])
            active_job_ids = [job["task_id"] for job in active_jobs]
            
            # Verify our jobs are in the active list
            for job_id in job_ids:
                assert job_id in active_job_ids
            
            # 3. Monitor all jobs until completion
            max_wait_time = 120  # 2 minutes for multiple jobs
            start_time = time.time()
            completed_jobs = set()
            
            while time.time() - start_time < max_wait_time and len(completed_jobs) < 3:
                for job_id in job_ids:
                    if job_id in completed_jobs:
                        continue
                        
                    status_request = {
                        "tool_name": "damien_job_get_status",
                        "parameters": {"job_id": job_id}
                    }
                    
                    status_response = await client.post(
                        f"{api_base_url}/tools/execute",
                        headers=auth_headers,
                        json=status_request,
                        timeout=10.0
                    )
                    
                    if status_response.status_code == 200:
                        status_result = status_response.json()
                        if status_result.get("status") == "completed":
                            completed_jobs.add(job_id)
                            print(f"Job {job_id} completed")
                        elif status_result.get("status") == "failed":
                            pytest.fail(f"Job {job_id} failed: {status_result}")
                
                await asyncio.sleep(3)  # Check every 3 seconds
            
            assert len(completed_jobs) == 3, f"Only {len(completed_jobs)} of 3 jobs completed"
            
            # 4. Verify all jobs have valid results
            for job_id in completed_jobs:
                result_request = {
                    "tool_name": "damien_job_get_result",
                    "parameters": {"job_id": job_id}
                }
                
                result_response = await client.post(
                    f"{api_base_url}/tools/execute",
                    headers=auth_headers,
                    json=result_request,
                    timeout=10.0
                )
                
                assert result_response.status_code == 200
                final_result = result_response.json()
                assert final_result.get("success") is True
                
                job_result = final_result.get("result")
                assert job_result is not None
                assert "emails_analyzed" in job_result

    @pytest.mark.asyncio
    async def test_error_handling_invalid_job_operations(self, api_base_url, auth_headers):
        """Test error handling for invalid job operations."""
        async with httpx.AsyncClient() as client:
            # 1. Test status check for non-existent job
            status_request = {
                "tool_name": "damien_job_get_status",
                "parameters": {"job_id": "non_existent_job_123"}
            }
            
            response = await client.post(
                f"{api_base_url}/tools/execute",
                headers=auth_headers,
                json=status_request,
                timeout=10.0
            )
            
            assert response.status_code == 200
            result = response.json()
            assert result.get("success") is False
            assert "not found" in result.get("error", "").lower()
            
            # 2. Test result retrieval for non-existent job
            result_request = {
                "tool_name": "damien_job_get_result",
                "parameters": {"job_id": "non_existent_job_456"}
            }
            
            response = await client.post(
                f"{api_base_url}/tools/execute",
                headers=auth_headers,
                json=result_request,
                timeout=10.0
            )
            
            result = response.json()
            assert result.get("success") is False
            assert "not found" in result.get("error", "").lower()
            
            # 3. Test cancellation of non-existent job
            cancel_request = {
                "tool_name": "damien_job_cancel",
                "parameters": {"job_id": "non_existent_job_789"}
            }
            
            response = await client.post(
                f"{api_base_url}/tools/execute",
                headers=auth_headers,
                json=cancel_request,
                timeout=10.0
            )
            
            result = response.json()
            assert result.get("success") is False
            assert "not found" in result.get("error", "").lower()
            
            # 4. Test missing job_id parameter
            invalid_request = {
                "tool_name": "damien_job_get_status",
                "parameters": {}  # Missing job_id
            }
            
            response = await client.post(
                f"{api_base_url}/tools/execute",
                headers=auth_headers,
                json=invalid_request,
                timeout=10.0
            )
            
            result = response.json()
            assert result.get("success") is False
            assert "required" in result.get("error", "").lower()

    @pytest.mark.asyncio 
    async def test_job_tracking_and_progress_monitoring(self, api_base_url, auth_headers):
        """Test detailed job tracking and progress monitoring."""
        async with httpx.AsyncClient() as client:
            # Start a job with moderate size for progress tracking
            job_request = {
                "tool_name": "damien_ai_analyze_emails_async",
                "parameters": {
                    "days": 3,
                    "target_count": 100,
                    "min_confidence": 0.8
                }
            }
            
            response = await client.post(
                f"{api_base_url}/tools/execute",
                headers=auth_headers,
                json=job_request,
                timeout=30.0
            )
            
            result = response.json()
            job_id = result.get("job_id")
            
            # Track progress over time
            progress_history = []
            max_checks = 30  # Maximum number of progress checks
            check_count = 0
            
            while check_count < max_checks:
                status_request = {
                    "tool_name": "damien_job_get_status",
                    "parameters": {"job_id": job_id}
                }
                
                status_response = await client.post(
                    f"{api_base_url}/tools/execute",
                    headers=auth_headers,
                    json=status_request,
                    timeout=10.0
                )
                
                status_result = status_response.json()
                job_status = status_result.get("status")
                progress_info = status_result.get("progress", {})
                
                progress_entry = {
                    "check_number": check_count,
                    "status": job_status,
                    "progress_percentage": progress_info.get("percentage", 0),
                    "message": progress_info.get("message", ""),
                    "timestamp": time.time()
                }
                progress_history.append(progress_entry)
                
                print(f"Progress check {check_count}: {job_status} - {progress_info.get('percentage', 0)}%")
                
                if job_status in ["completed", "failed", "cancelled"]:
                    break
                    
                check_count += 1
                await asyncio.sleep(2)
            
            # Validate progress tracking
            assert len(progress_history) > 0
            
            # Check that we captured the completion
            final_status = progress_history[-1]["status"]
            assert final_status in ["completed", "failed", "cancelled"]
            
            # If completed, verify we can retrieve results
            if final_status == "completed":
                result_request = {
                    "tool_name": "damien_job_get_result",
                    "parameters": {"job_id": job_id}
                }
                
                result_response = await client.post(
                    f"{api_base_url}/tools/execute",
                    headers=auth_headers,
                    json=result_request,
                    timeout=10.0
                )
                
                final_result = result_response.json()
                assert final_result.get("success") is True
                
                print(f"Job completed with result: {final_result.get('result')}")

    @pytest.mark.asyncio
    async def test_job_list_functionality(self, api_base_url, auth_headers):
        """Test job listing functionality for active task management."""
        async with httpx.AsyncClient() as client:
            # 1. Get initial job list (should be empty or contain only old jobs)
            initial_list_request = {
                "tool_name": "damien_job_list",
                "parameters": {}
            }
            
            response = await client.post(
                f"{api_base_url}/tools/execute",
                headers=auth_headers,
                json=initial_list_request,
                timeout=10.0
            )
            
            initial_result = response.json()
            initial_count = initial_result.get("count", 0)
            
            # 2. Start a new job
            job_request = {
                "tool_name": "damien_ai_analyze_emails_async",
                "parameters": {
                    "days": 1,
                    "target_count": 50,
                    "min_confidence": 0.7
                }
            }
            
            job_response = await client.post(
                f"{api_base_url}/tools/execute",
                headers=auth_headers,
                json=job_request,
                timeout=30.0
            )
            
            job_result = job_response.json()
            new_job_id = job_result.get("job_id")
            
            # 3. Check that job appears in list
            await asyncio.sleep(1)  # Brief wait for job to register
            
            updated_list_request = {
                "tool_name": "damien_job_list",
                "parameters": {}
            }
            
            list_response = await client.post(
                f"{api_base_url}/tools/execute",
                headers=auth_headers,
                json=updated_list_request,
                timeout=10.0
            )
            
            updated_result = list_response.json()
            assert updated_result.get("success") is True
            
            updated_count = updated_result.get("count", 0)
            active_jobs = updated_result.get("active_jobs", [])
            
            # Verify our job is in the list
            job_found = False
            for job in active_jobs:
                if job.get("task_id") == new_job_id:
                    job_found = True
                    assert job.get("name") is not None
                    assert job.get("status") in ["pending", "running"]
                    break
            
            assert job_found, f"Job {new_job_id} not found in active jobs list"
            
            # 4. Wait for job completion and verify it's removed from active list
            max_wait = 60
            start_time = time.time()
            
            while time.time() - start_time < max_wait:
                final_list_request = {
                    "tool_name": "damien_job_list", 
                    "parameters": {}
                }
                
                final_response = await client.post(
                    f"{api_base_url}/tools/execute",
                    headers=auth_headers,
                    json=final_list_request,
                    timeout=10.0
                )
                
                final_result = final_response.json()
                final_active_jobs = final_result.get("active_jobs", [])
                
                # Check if our job is still in active list
                job_still_active = any(job.get("task_id") == new_job_id for job in final_active_jobs)
                
                if not job_still_active:
                    print(f"Job {new_job_id} successfully removed from active list")
                    break
                    
                await asyncio.sleep(3)
            
            # Final verification - job should no longer be active
            final_active_jobs = final_result.get("active_jobs", [])
            job_still_active = any(job.get("task_id") == new_job_id for job in final_active_jobs)
            assert not job_still_active, f"Job {new_job_id} should not be in active list after completion"


class TestJobManagementPerformance:
    """Performance tests for job management system."""
    
    @pytest.mark.asyncio
    async def test_job_creation_performance(self, api_base_url, auth_headers):
        """Test that job creation is fast and responsive."""
        async with httpx.AsyncClient() as client:
            start_time = time.time()
            
            job_request = {
                "tool_name": "damien_ai_analyze_emails_async",
                "parameters": {
                    "days": 1,
                    "target_count": 10
                }
            }
            
            response = await client.post(
                f"{api_base_url}/tools/execute",
                headers=auth_headers,
                json=job_request,
                timeout=30.0
            )
            
            creation_time = time.time() - start_time
            
            assert response.status_code == 200
            result = response.json()
            assert result.get("success") is True
            
            # Job creation should be very fast (< 5 seconds)
            assert creation_time < 5.0, f"Job creation took {creation_time:.2f} seconds, should be < 5s"
            
            print(f"Job created in {creation_time:.3f} seconds")

    @pytest.mark.asyncio 
    async def test_status_check_responsiveness(self, api_base_url, auth_headers):
        """Test that status checks are fast and don't block."""
        async with httpx.AsyncClient() as client:
            # Create a job first
            job_request = {
                "tool_name": "damien_ai_analyze_emails_async",
                "parameters": {"days": 1, "target_count": 20}
            }
            
            response = await client.post(
                f"{api_base_url}/tools/execute",
                headers=auth_headers,
                json=job_request,
                timeout=30.0
            )
            
            job_id = response.json().get("job_id")
            
            # Test multiple rapid status checks
            status_times = []
            
            for i in range(5):
                start_time = time.time()
                
                status_request = {
                    "tool_name": "damien_job_get_status",
                    "parameters": {"job_id": job_id}
                }
                
                status_response = await client.post(
                    f"{api_base_url}/tools/execute",
                    headers=auth_headers,
                    json=status_request,
                    timeout=10.0
                )
                
                status_time = time.time() - start_time
                status_times.append(status_time)
                
                assert status_response.status_code == 200
                
            # All status checks should be fast (< 2 seconds)
            max_status_time = max(status_times)
            avg_status_time = sum(status_times) / len(status_times)
            
            assert max_status_time < 2.0, f"Slowest status check: {max_status_time:.2f}s"
            assert avg_status_time < 1.0, f"Average status check: {avg_status_time:.2f}s"
            
            print(f"Status checks - Max: {max_status_time:.3f}s, Avg: {avg_status_time:.3f}s")


if __name__ == "__main__":
    # Note: These tests require the MCP server to be running
    print("Integration tests for job management workflow")
    print("Ensure MCP server is running on localhost:8892")
    pytest.main([__file__, "-v", "--tb=short"])
                status_icon = "‚úÖ" if success else "‚ùå"
                print(f"{status_icon} {test_name}: {details}")
            
            print("-" * 50)
            success_rate = (passed_tests / total_tests) * 100 if total_tests > 0 else 0
            print(f"üéØ Overall Success Rate: {passed_tests}/{total_tests} ({success_rate:.1f}%)")
            
            if success_rate >= 80:
                print("üèÜ Job Management Validation: PASSED")
                print("   System ready for Phase 1 completion")
                return True
            else:
                print("‚ö†Ô∏è  Job Management Validation: NEEDS ATTENTION")
                print("   Some tests failed - review and fix issues")
                return False
                
        except Exception as e:
            print(f"‚ùå Fatal error during validation: {e}")
            return False


async def main():
    """Main entry point for job management validation."""
    print("ü§º‚Äç‚ôÇÔ∏è Damien Email Wrestler - Job Management Validation")
    print("Building on proven async infrastructure that processed 3,500 emails in 7 minutes")
    print()
    
    # Check if server URL provided
    import os
    server_url = os.getenv("DAMIEN_MCP_SERVER_URL", "http://localhost:8892")
    api_key = os.getenv("DAMIEN_MCP_SERVER_API_KEY", "test-api-key")
    
    print(f"üîó Connecting to: {server_url}")
    print(f"üîë Using API key: {api_key[:8]}..." if len(api_key) > 8 else "üîë Using API key: [short key]")
    print()
    
    validator = JobManagementValidator(server_url, api_key)
    success = await validator.run_all_tests()
    
    if success:
        print()
        print("üéâ All tests passed! Job management infrastructure is validated.")
        print("‚úÖ Ready to proceed to Phase 1: Concurrent Job Processing Stress Test")
        sys.exit(0)
    else:
        print()
        print("‚ùå Some tests failed. Please review and fix issues before proceeding.")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
